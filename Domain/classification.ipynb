{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:05.431832Z",
     "start_time": "2025-01-05T09:50:05.382053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "# import nltk\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "#from nltk.book import texts\n",
    "from autocorrect import Speller\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    def character_normalization(text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "    def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                          flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match) \\\n",
    "                if contraction_mapping.get(match) \\\n",
    "                else contraction_mapping.get(match.lower())\n",
    "            expanded_contraction = first_char + expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "    def remove_extra_new_lines(text):\n",
    "        return re.sub(r'[\\r|\\n|\\r\\n]+', ' ', text)\n",
    "\n",
    "    def case_conversion(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def autocorrect(text):\n",
    "        spell = Speller(fast=True)\n",
    "        return spell(text)\n",
    "\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        if is_lower_case:\n",
    "            filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "        else:\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "\n",
    "    def simple_stemmer(text):\n",
    "        ps = nltk.porter.PorterStemmer()\n",
    "        text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "    def special_char_removal(text, remove_digits=False):\n",
    "        def remove_special_characters(text, remove_digits):\n",
    "            pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "            text = re.sub(pattern, '', text)\n",
    "            return text\n",
    "\n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "        return remove_special_characters(text, remove_digits)\n",
    "\n",
    "    def extra_white_space_removal(text):\n",
    "        return re.sub(' +', ' ', text)\n",
    "\n",
    "    # Apply the preprocessing steps\n",
    "    text = character_normalization(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = case_conversion(text)\n",
    "    text = remove_extra_new_lines(text)\n",
    "    # text = simple_stemmer(text)\n",
    "    text = lemmatize_text(text)\n",
    "    text = special_char_removal(text, remove_digits=True)\n",
    "    text = extra_white_space_removal(text)\n",
    "    text = remove_stopwords(text, is_lower_case=True, stopwords=stopword_list)\n",
    "    text = autocorrect(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_df(file_path):\n",
    "    # reading the data\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split each line on the tab character\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:  # Ensure the line has exactly two parts\n",
    "                data.append(parts)\n",
    "            # Convert the processed data into a DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "            \n",
    "# Importing data\n",
    "df_amazon = text_to_df('../Data/sentiment labelled sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "df_imdb = text_to_df('../Data/sentiment labelled sentences/imdb_labelled.txt')\n",
    "\n",
    "\n",
    "\n",
    "df_yelp = text_to_df('../Data/sentiment labelled sentences/yelp_labelled.txt')\n",
    "df = pd.concat([df_yelp, df_imdb, df_amazon], ignore_index=True)\n",
    "df.columns = ['Review', 'Score']\n",
    "print(df)\n",
    "\n",
    "# Check if preprocessed pickle file exists\n",
    "pickle_file = 'preprocessed_dataframe.pkl'\n",
    "\n",
    "if os.path.exists(pickle_file):\n",
    "    # Load the DataFrame from the pickle file\n",
    "    df = pd.read_pickle(pickle_file)\n",
    "else:\n",
    "    df['Review'] = df['Review'].apply(preprocess_text)\n",
    "    # Save the preprocessed DataFrame as a pickle file\n",
    "    df.to_pickle(pickle_file)\n",
    "\n",
    "print(df)"
   ],
   "id": "506f46b14c71a272",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Score\n",
      "0                              Wow... Loved this place.     1\n",
      "1                                    Crust is not good.     0\n",
      "2             Not tasty and the texture was just nasty.     0\n",
      "3     Stopped by during the late May bank holiday of...     1\n",
      "4     The selection on the menu was great and so wer...     1\n",
      "...                                                 ...   ...\n",
      "2995  The screen does get smudged easily because it ...     0\n",
      "2996  What a piece of junk.. I lose more calls on th...     0\n",
      "2997                       Item Does Not Match Picture.     0\n",
      "2998  The only thing that disappoint me is the infra...     0\n",
      "2999  You can not answer calls with the unit, never ...     0\n",
      "\n",
      "[3000 rows x 2 columns]\n",
      "                                                 Review Score\n",
      "0                                       wow loved place     1\n",
      "1                                            crust good     0\n",
      "2                                tasty texture wa nasty     0\n",
      "3     stopped late may bank holiday rick steve recom...     1\n",
      "4                        selection menu wa great prices     1\n",
      "...                                                 ...   ...\n",
      "2995       screen doe get smudged easily touch ear face     0\n",
      "2996                         piece junk lose call phone     0\n",
      "2997                             item doe match picture     0\n",
      "2998               thing disappoint infra red port rida     0\n",
      "2999                      answer call unit never worked     0\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:05.683703Z",
     "start_time": "2025-01-05T09:50:05.681053Z"
    }
   },
   "cell_type": "code",
   "source": "# df.to_pickle(\"preprocessed_dataframe.pkl\")",
   "id": "aafb291f8a032a9d",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:05.702407Z",
     "start_time": "2025-01-05T09:50:05.694786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if the pickle file exists\n",
    "pickle_file = 'preprocessed_dataframe.pkl'\n",
    "\n",
    "if os.path.exists(pickle_file):\n",
    "    # Load the DataFrame from the pickle file\n",
    "    df = pd.read_pickle(pickle_file)\n",
    "else:\n",
    "    # Importing data\n",
    "    df_list = [text_to_df(file_path) for file_path in file_paths]\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.columns = ['Review', 'Score']\n",
    "    \n",
    "    # Preprocessing\n",
    "    df['Review'] = df['Review'].apply(preprocess_text)\n",
    "    \n",
    "    # Save the preprocessed DataFrame as a pickle file\n",
    "    df.to_pickle(pickle_file)\n",
    "\n",
    "print(df)"
   ],
   "id": "a31a12d57e58d361",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Score\n",
      "0                                       wow loved place     1\n",
      "1                                            crust good     0\n",
      "2                                tasty texture wa nasty     0\n",
      "3     stopped late may bank holiday rick steve recom...     1\n",
      "4                        selection menu wa great prices     1\n",
      "...                                                 ...   ...\n",
      "2995       screen doe get smudged easily touch ear face     0\n",
      "2996                         piece junk lose call phone     0\n",
      "2997                             item doe match picture     0\n",
      "2998               thing disappoint infra red port rida     0\n",
      "2999                      answer call unit never worked     0\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:06.177548Z",
     "start_time": "2025-01-05T09:50:06.175186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import nltk\n",
    "# nltk.download('all')"
   ],
   "id": "7f4f594210a5df11",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:06.191288Z",
     "start_time": "2025-01-05T09:50:06.188368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# # Tokenization\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# \n",
    "# # Ensure NLTK tokenizers are downloaded\n",
    "# nltk.download('punkt')\n",
    "# \n",
    "# # Tokenize each review into a list of words\n",
    "# df['Tokens'] = df['Review'].apply(word_tokenize)\n",
    " "
   ],
   "id": "3cc0964b2e77c128",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:06.401770Z",
     "start_time": "2025-01-05T09:50:06.395703Z"
    }
   },
   "cell_type": "code",
   "source": "df  ",
   "id": "7005e4eb5a6d1b64",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                                 Review Score\n",
       "0                                       wow loved place     1\n",
       "1                                            crust good     0\n",
       "2                                tasty texture wa nasty     0\n",
       "3     stopped late may bank holiday rick steve recom...     1\n",
       "4                        selection menu wa great prices     1\n",
       "...                                                 ...   ...\n",
       "2995       screen doe get smudged easily touch ear face     0\n",
       "2996                         piece junk lose call phone     0\n",
       "2997                             item doe match picture     0\n",
       "2998               thing disappoint infra red port rida     0\n",
       "2999                      answer call unit never worked     0\n",
       "\n",
       "[3000 rows x 2 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wow loved place</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>crust good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tasty texture wa nasty</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stopped late may bank holiday rick steve recom...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selection menu wa great prices</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>screen doe get smudged easily touch ear face</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>piece junk lose call phone</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>item doe match picture</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>thing disappoint infra red port rida</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>answer call unit never worked</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows Ã— 2 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:06.641621Z",
     "start_time": "2025-01-05T09:50:06.639834Z"
    }
   },
   "cell_type": "code",
   "source": "# glove_pretrained = gensim.downloader.load('glove-wiki-gigaword-300')\n",
   "id": "a614266cac492a88",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:06.665675Z",
     "start_time": "2025-01-05T09:50:06.663319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Load the GloVe embeddings\n",
    "# def load_glove_embeddings(file_path):\n",
    "#     embeddings_index = {}\n",
    "#     with open(file_path, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#     return embeddings_index\n",
    "# \n",
    "# glove_embeddings = load_glove_embeddings('glove.6B.300d.txt')\n",
    "# \n",
    "# # Convert text to GloVe embeddings\n",
    "# def get_glove_embedding(text, embeddings, embedding_dim=300):\n",
    "#     words = text.split()\n",
    "#     embedding = np.zeros(embedding_dim)\n",
    "#     count = 0\n",
    "#     for word in words:\n",
    "#         if word in embeddings:\n",
    "#             embedding += embeddings[word]\n",
    "#             count += 1\n",
    "#     if count > 0:\n",
    "#         embedding /= count\n",
    "#     return embedding\n",
    "# \n",
    "# # Convert tokens to GloVe embeddings\n",
    "# df['Embedding'] = df['Tokens'].apply(lambda tokens: get_glove_embedding(' '.join(tokens), glove_embeddings))\n",
    "# X = np.vstack(df['Embedding'].values)\n",
    "# y = df['Score'].astype(int).values  # Convert to a NumPy array\n",
    "\n"
   ],
   "id": "9c4260b18d3c5184",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T09:50:06.724116Z",
     "start_time": "2025-01-05T09:50:06.690836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Prepare the data\n",
    "X = df['Review']  # Feature: Sentences\n",
    "y = df['Score']     # Target: Sentiment scores\n",
    "\n",
    "# Step 2: Create a Bag-of-Words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(X)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "\n",
    "# Train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separating training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
    "y = df['Score'].astype(int).values\n",
    "\n",
    "# Fit the GridSearchCV\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and accuracy\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "accuracy = best_rf.score(X_test, y_test)\n",
    "print(f\"Test Set Accuracy: {accuracy}\")"
   ],
   "id": "fd3e08c08746813a",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f14c40758892ad92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
