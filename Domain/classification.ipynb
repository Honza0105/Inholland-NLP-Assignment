{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:18:17.434989Z",
     "start_time": "2025-01-05T14:18:17.387368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "# import nltk\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import nltk\n",
    "\n",
    "from nltk import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "#from nltk.book import texts\n",
    "from autocorrect import Speller\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "def preprocess_text(text, use_lemmatization = True):\n",
    "    def character_normalization(text):\n",
    "        text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        return text\n",
    "\n",
    "    def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "        contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                          flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "        def expand_match(contraction):\n",
    "            match = contraction.group(0)\n",
    "            first_char = match[0]\n",
    "            expanded_contraction = contraction_mapping.get(match) \\\n",
    "                if contraction_mapping.get(match) \\\n",
    "                else contraction_mapping.get(match.lower())\n",
    "            expanded_contraction = first_char + expanded_contraction[1:]\n",
    "            return expanded_contraction\n",
    "\n",
    "        expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "        expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "        return expanded_text\n",
    "\n",
    "    def remove_extra_new_lines(text):\n",
    "        return re.sub(r'[\\r|\\n|\\r\\n]+', ' ', text)\n",
    "\n",
    "    def case_conversion(text):\n",
    "        return text.lower()\n",
    "\n",
    "    def autocorrect(text):\n",
    "        spell = Speller(fast=True)\n",
    "        return spell(text)\n",
    "\n",
    "    tokenizer = ToktokTokenizer()\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    # Keeping not to preserve negative sentiment\n",
    "    stopword_list.remove('not')\n",
    "\n",
    "    def remove_stopwords(text, is_lower_case=False, stopwords=stopword_list):\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        if is_lower_case:\n",
    "            filtered_tokens = [token for token in tokens if token not in stopwords]\n",
    "        else:\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
    "        filtered_text = ' '.join(filtered_tokens)\n",
    "        return filtered_text\n",
    "\n",
    "    def simple_stemmer(text):\n",
    "        ps = nltk.porter.PorterStemmer()\n",
    "        text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "        return text\n",
    "    \n",
    "\n",
    "    def lemmatize_text(text):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "    def special_char_removal(text, remove_digits=False):\n",
    "        def remove_special_characters(text, remove_digits):\n",
    "            pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "            text = re.sub(pattern, '', text)\n",
    "            return text\n",
    "\n",
    "        special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "        text = special_char_pattern.sub(\" \\\\1 \", text)\n",
    "        return remove_special_characters(text, remove_digits)\n",
    "\n",
    "    def extra_white_space_removal(text):\n",
    "        return re.sub(' +', ' ', text)\n",
    "\n",
    "    # Apply the preprocessing steps\n",
    "    text = character_normalization(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = case_conversion(text)\n",
    "    text = remove_extra_new_lines(text)\n",
    "    # text = simple_stemmer(text)\n",
    "    if use_lemmatization:\n",
    "        text = lemmatize_text(text)\n",
    "    text = special_char_removal(text, remove_digits=True)\n",
    "    text = extra_white_space_removal(text)\n",
    "    text = remove_stopwords(text, is_lower_case=True, stopwords=stopword_list)\n",
    "    text = autocorrect(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_to_df(file_path):\n",
    "    # reading the data\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Split each line on the tab character\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:  # Ensure the line has exactly two parts\n",
    "                data.append(parts)\n",
    "            # Convert the processed data into a DataFrame\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def train_random_forest(df, vectorizer=CountVectorizer(), use_embedding=False, X_embeddings = None):\n",
    "    # Step 1: Prepare the data\n",
    "    X = df['Review']  # Feature: Sentences\n",
    "    y = df['Score']     # Target: Sentiment scores\n",
    "    \n",
    "    # Step 2: Create a Bag-of-Words representation or other passed representation\n",
    "    if use_embedding:\n",
    "        X = X_embeddings\n",
    "    else:\n",
    "        X = vectorizer.fit_transform(X)\n",
    "    \n",
    "    \n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    # Define the model\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Train-test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Separating training and testing data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    y = df['Score'].astype(int).values\n",
    "    \n",
    "    # Fit the GridSearchCV\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters and accuracy\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    accuracy = best_rf.score(X_test, y_test)\n",
    "    print(f\"Test Set Accuracy: {accuracy}\")\n",
    "    return accuracy\n",
    "            "
   ],
   "id": "c41db658105c831a",
   "outputs": [],
   "execution_count": 173
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:52:58.423535Z",
     "start_time": "2025-01-05T13:51:38.220592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing data\n",
    "df_amazon = text_to_df('../Data/sentiment labelled sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "df_imdb = text_to_df('../Data/sentiment labelled sentences/imdb_labelled.txt')\n",
    "\n",
    "\n",
    "\n",
    "df_yelp = text_to_df('../Data/sentiment labelled sentences/yelp_labelled.txt')\n",
    "df = pd.concat([df_yelp, df_imdb, df_amazon], ignore_index=True)\n",
    "df.columns = ['Review', 'Score']\n",
    "print(df)\n",
    "\n",
    "# Check if preprocessed pickle file exists\n",
    "pickle_file = 'preprocessed_dataframe.pkl'\n",
    "\n",
    "if os.path.exists(pickle_file):\n",
    "    # Load the DataFrame from the pickle file\n",
    "    df = pd.read_pickle(pickle_file)\n",
    "else:\n",
    "    df['Review'] = df['Review'].apply(preprocess_text)\n",
    "    # Save the preprocessed DataFrame as a pickle file\n",
    "    df.to_pickle(pickle_file)\n",
    "\n",
    "print(df)"
   ],
   "id": "506f46b14c71a272",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Score\n",
      "0                              Wow... Loved this place.     1\n",
      "1                                    Crust is not good.     0\n",
      "2             Not tasty and the texture was just nasty.     0\n",
      "3     Stopped by during the late May bank holiday of...     1\n",
      "4     The selection on the menu was great and so wer...     1\n",
      "...                                                 ...   ...\n",
      "2995  The screen does get smudged easily because it ...     0\n",
      "2996  What a piece of junk.. I lose more calls on th...     0\n",
      "2997                       Item Does Not Match Picture.     0\n",
      "2998  The only thing that disappoint me is the infra...     0\n",
      "2999  You can not answer calls with the unit, never ...     0\n",
      "\n",
      "[3000 rows x 2 columns]\n",
      "                                                 Review Score\n",
      "0                                       wow loved place     1\n",
      "1                                        crust not good     0\n",
      "2                            not tasty texture wa nasty     0\n",
      "3     stopped late may bank holiday rick steve recom...     1\n",
      "4                        selection menu wa great prices     1\n",
      "...                                                 ...   ...\n",
      "2995       screen doe get smudged easily touch ear face     0\n",
      "2996                         piece junk lose call phone     0\n",
      "2997                         item doe not match picture     0\n",
      "2998               thing disappoint infra red port rida     0\n",
      "2999                  not answer call unit never worked     0\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 160
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:52:58.630670Z",
     "start_time": "2025-01-05T13:52:58.628057Z"
    }
   },
   "cell_type": "code",
   "source": "print(df)",
   "id": "1c9ef85c349577d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Score\n",
      "0                                       wow loved place     1\n",
      "1                                        crust not good     0\n",
      "2                            not tasty texture wa nasty     0\n",
      "3     stopped late may bank holiday rick steve recom...     1\n",
      "4                        selection menu wa great prices     1\n",
      "...                                                 ...   ...\n",
      "2995       screen doe get smudged easily touch ear face     0\n",
      "2996                         piece junk lose call phone     0\n",
      "2997                         item doe not match picture     0\n",
      "2998               thing disappoint infra red port rida     0\n",
      "2999                  not answer call unit never worked     0\n",
      "\n",
      "[3000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 161
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:56:11.209033Z",
     "start_time": "2025-01-05T13:56:11.195187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "    "
   ],
   "id": "c29ba0ec7847e21b",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:53:32.676624Z",
     "start_time": "2025-01-05T13:52:58.782537Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n",
      "Best Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.8141666666666666\n",
      "Test Set Accuracy: 0.7966666666666666\n"
     ]
    }
   ],
   "execution_count": 162,
   "source": "original_model_accuracy = train_random_forest(df)",
   "id": "a31a12d57e58d361"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:53:32.995735Z",
     "start_time": "2025-01-05T13:53:32.994519Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3b07259a7607cd4b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 3",
   "id": "397bfeaa02448650"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Lemma ",
   "id": "acfede01221d99c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:55:34.261909Z",
     "start_time": "2025-01-05T13:53:33.011144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Importing data\n",
    "df_amazon = text_to_df('../Data/sentiment labelled sentences/amazon_cells_labelled.txt')\n",
    "\n",
    "df_imdb = text_to_df('../Data/sentiment labelled sentences/imdb_labelled.txt')\n",
    "\n",
    "\n",
    "\n",
    "df_yelp = text_to_df('../Data/sentiment labelled sentences/yelp_labelled.txt')\n",
    "df_lemma = pd.concat([df_yelp, df_imdb, df_amazon], ignore_index=True)\n",
    "df_lemma.columns = ['Review', 'Score']\n",
    "\n",
    "# Check if preprocessed pickle file exists\n",
    "pickle_file = 'preprocessed_dataframe_no_lemma.pkl'\n",
    "\n",
    "if os.path.exists(pickle_file):\n",
    "    # Load the DataFrame from the pickle file\n",
    "    df_lemma = pd.read_pickle(pickle_file)\n",
    "else:\n",
    "    df_lemma['Review'] = df_lemma['Review'].apply(preprocess_text, args=(False,))\n",
    "    # Save the preprocessed DataFrame as a pickle file\n",
    "    df_lemma.to_pickle(pickle_file)\n",
    "\n",
    "print(df_lemma)\n",
    "\n",
    "no_lemma_model_accuracy = train_random_forest(df_lemma)\n",
    "print(f\"No lemmatization accuracy: {no_lemma_model_accuracy}\")\n"
   ],
   "id": "f14c40758892ad92",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Score\n",
      "0                                       wow loved place     1\n",
      "1                                        crust not good     0\n",
      "2                               not tasty texture nasty     0\n",
      "3     stopped late may bank holiday rick steve recom...     1\n",
      "4                           selection menu great prices     1\n",
      "...                                                 ...   ...\n",
      "2995         screen get smudged easily touches ear face     0\n",
      "2996                        piece junk lose calls phone     0\n",
      "2997                             item not match picture     0\n",
      "2998               thing disappoint infra red port rida     0\n",
      "2999                 not answer calls unit never worked     0\n",
      "\n",
      "[3000 rows x 2 columns]\n",
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janjelinek/Library/CloudStorage/OneDrive-HogeschoolInholland/Inholland/Python/AM_machine_learning_project/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.8304166666666667\n",
      "Test Set Accuracy: 0.8016666666666666\n",
      "No lemmatization accuracy: {no_lemma_model_accuracy}\n"
     ]
    }
   ],
   "execution_count": 163
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## tf.idf",
   "id": "8c34ea7fd7a6600"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:57:50.399009Z",
     "start_time": "2025-01-05T13:57:19.369842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_model_accuracy = train_random_forest(df, vectorizer=TfidfVectorizer())\n",
    "print(f\"TF.IDF model accuracy: {no_lemma_model_accuracy}\")\n",
    "\n"
   ],
   "id": "a7548413e6076f04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janjelinek/Library/CloudStorage/OneDrive-HogeschoolInholland/Inholland/Python/AM_machine_learning_project/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "Best Cross-Validation Accuracy: 0.8166666666666668\n",
      "Test Set Accuracy: 0.8133333333333334\n",
      "TF.IDF model accuracy: 0.8016666666666666\n"
     ]
    }
   ],
   "execution_count": 171
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## n-grams",
   "id": "ab962f7108ad49e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:06:41.231072Z",
     "start_time": "2025-01-05T14:05:30.660196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "tf_idf_model_accuracy = train_random_forest(df, vectorizer=CountVectorizer(ngram_range=(1, 3)))\n",
    "print(f\"1-3-Gram model accuracy: {no_lemma_model_accuracy}\")\n",
    "\n"
   ],
   "id": "44e5b5ce0924573d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janjelinek/Library/CloudStorage/OneDrive-HogeschoolInholland/Inholland/Python/AM_machine_learning_project/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.8095833333333333\n",
      "Test Set Accuracy: 0.795\n",
      "1-3-Gram model accuracy: 0.8016666666666666\n"
     ]
    }
   ],
   "execution_count": 172
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## embedding",
   "id": "844827ad6e0388f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:55:34.911213Z",
     "start_time": "2025-01-05T13:55:34.909328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "# \n",
    "# # Load the GloVe embeddings\n",
    "# def load_glove_embeddings(file_path):\n",
    "#     embeddings_index = {}\n",
    "#     with open(file_path, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#     return embeddings_index\n",
    "# \n",
    "# glove_embeddings = load_glove_embeddings('glove.6B.300d.txt')\n",
    "# \n",
    "# # Convert text to GloVe embeddings\n",
    "# def get_glove_embedding(text, embeddings, embedding_dim=300):\n",
    "#     words = text.split()\n",
    "#     embedding = np.zeros(embedding_dim)\n",
    "#     count = 0\n",
    "#     for word in words:\n",
    "#         if word in embeddings:\n",
    "#             embedding += embeddings[word]\n",
    "#             count += 1\n",
    "#     if count > 0:\n",
    "#         embedding /= count\n",
    "#     return embedding\n",
    "# \n",
    "# # Convert tokens to GloVe embeddings\n",
    "# df['Embedding'] = df['Tokens'].apply(lambda tokens: get_glove_embedding(' '.join(tokens), glove_embeddings))\n",
    "# X = np.vstack(df['Embedding'].values)\n",
    "# y = df['Score'].astype(int).values  # Convert to a NumPy array\n",
    "\n"
   ],
   "id": "9c17b07b9e553da2",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T14:21:50.569560Z",
     "start_time": "2025-01-05T14:18:48.808181Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(glove_file):\n",
    "    glove_embeddings = {}\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype='float32')\n",
    "            glove_embeddings[word] = vector\n",
    "    return glove_embeddings\n",
    "\n",
    "# Load the GloVe embeddings (adjust the path as needed)\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.300d.txt')\n",
    "\n",
    "def get_average_glove_embedding(review, glove_embeddings):\n",
    "    words = review.split()\n",
    "    word_vectors = [glove_embeddings[word] for word in words if word in glove_embeddings]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(300)  # Adjust the size based on the GloVe dimension (300 in this case)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "# Create a new feature matrix using average GloVe embeddings\n",
    "def create_glove_embeddings(df, glove_embeddings):\n",
    "    return np.array([get_average_glove_embedding(review, glove_embeddings) for review in df['Review']])\n",
    "\n",
    "\n",
    "# Preprocess the text\n",
    "df['Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Create GloVe embeddings\n",
    "X_embeddings = create_glove_embeddings(df, glove_embeddings)\n",
    "\n",
    "# Train the Random Forest model using the embeddings\n",
    "accuracy = train_random_forest(df, use_embedding=True, X_embeddings=X_embeddings)\n",
    "print(f\"GloVe model accuracy: {accuracy}\")\n"
   ],
   "id": "597fed41b0653a4d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 162 candidates, totalling 486 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/janjelinek/Library/CloudStorage/OneDrive-HogeschoolInholland/Inholland/Python/AM_machine_learning_project/lib/python3.12/site-packages/numpy/ma/core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Best Cross-Validation Accuracy: 0.8112499999999999\n",
      "Test Set Accuracy: 0.7966666666666666\n",
      "GloVe model accuracy: 0.7966666666666666\n"
     ]
    }
   ],
   "execution_count": 174
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Named entity, etc",
   "id": "702e756c68285380"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-05T13:55:35.055826Z",
     "start_time": "2025-01-05T13:55:35.054219Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "17d98a6eb5fd80a2",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
